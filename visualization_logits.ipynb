{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "temp_df = {}\n",
    "for i in range(10):\n",
    "    temp_df['df_' + str(i)] = df[df['label'] == i].sample(5)\n",
    "\n",
    "concat_df = pd.concat(temp_df.values()).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = concat_df.drop('label', axis=1).values, concat_df['label'].values\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32) / 255.0\n",
    "Y = torch.tensor(Y, dtype=torch.long) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "W1 = torch.randn(784, 256) * 0.01\n",
    "b1 = torch.zeros(256)\n",
    "\n",
    "W2 = torch.empty(256, 10) \n",
    "init.xavier_uniform_(W2)\n",
    "\n",
    "b2 = torch.zeros(10)\n",
    "\n",
    "parameters = [W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "parameters = [W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_ = []\n",
    "W2_ = []\n",
    "\n",
    "H_ = []\n",
    "H_preact_ = []\n",
    "\n",
    "logits_ = []\n",
    "logits_gradients = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0/20000, loss: 2.3240225315093994\n",
      "iteration 1000/20000, loss: 0.002541974885389209\n",
      "iteration 2000/20000, loss: 0.0011715571163222194\n",
      "iteration 3000/20000, loss: 0.0007481785141862929\n",
      "iteration 4000/20000, loss: 0.0005452451296150684\n",
      "iteration 5000/20000, loss: 0.00042691529961302876\n",
      "iteration 6000/20000, loss: 0.00034976081224158406\n",
      "iteration 7000/20000, loss: 0.00029560725670307875\n",
      "iteration 8000/20000, loss: 0.00025559798814356327\n",
      "iteration 9000/20000, loss: 0.00022485379304271191\n",
      "iteration 10000/20000, loss: 0.00020052740001119673\n",
      "iteration 11000/20000, loss: 0.0001808411761885509\n",
      "iteration 12000/20000, loss: 0.0001645154698053375\n",
      "iteration 13000/20000, loss: 0.0001508639834355563\n",
      "iteration 14000/20000, loss: 0.0001392289123032242\n",
      "iteration 15000/20000, loss: 0.00012920751760248095\n",
      "iteration 16000/20000, loss: 0.0001204970758408308\n",
      "iteration 17000/20000, loss: 0.00011284735955996439\n",
      "iteration 18000/20000, loss: 0.00010608914453769103\n",
      "iteration 19000/20000, loss: 0.00010007938544731587\n"
     ]
    }
   ],
   "source": [
    "for i in range(20000):\n",
    "\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "  H_preact = X @ W1 + b1\n",
    "  H = torch.tanh(H_preact)\n",
    "  logits = H @ W2 + b2\n",
    "  loss = F.cross_entropy(logits, Y)\n",
    "\n",
    "  for j in [H_preact, W1, b1, H, W2, b2, logits]:\n",
    "      j.retain_grad()\n",
    "  loss.backward()\n",
    "\n",
    "  for params in parameters:\n",
    "     params.data += -0.1 * params.grad\n",
    "\n",
    "  W1_.append(W1.clone())\n",
    "  H_.append(H.clone())\n",
    "  H_preact_.append(H_preact.clone())\n",
    "  W2_.append(W2.clone())\n",
    "  logits_gradients.append(logits.grad.clone())\n",
    "  logits_.append(logits.clone())\n",
    "\n",
    "  if i % 1000 == 0:\n",
    "      print(f'iteration {i}/{20000}, loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d509f2db12bf433daeb6d9f4b7583d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='I', max=19999), Output()), _dom_classes=('widget-interacâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_plot(I)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "i_widget = widgets.IntSlider(min=0, max=19999, step=1, value=0)\n",
    "\n",
    "def update_plot(I):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.hist(W1_[I].view(-1).detach(), bins=50)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of W1')\n",
    "\n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.hist(H_preact_[I].view(-1).detach(), bins=50)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of H preact')\n",
    "\n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.hist(H_[I].view(-1).detach(), bins=50)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of H')\n",
    "\n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.hist(W2_[I].view(-1).detach(), bins=50)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of W2')\n",
    "\n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.imshow((F.softmax(logits_[I].detach(), dim=1)).T, cmap='gray')\n",
    "    plt.xlabel(f'Iteration : {I}')\n",
    "    plt.ylabel('Logits')\n",
    "    plt.title('Logits')\n",
    "\n",
    "    plt.subplot(3, 2, 6)\n",
    "    plt.imshow(logits_gradients[I].detach().T, cmap='gray')\n",
    "    plt.xlabel(f'Iteration : {I}')\n",
    "    plt.title('Logits Gradients')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "widgets.interact(update_plot, I=i_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
